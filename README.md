# ğŸ¡ Zoopla Property Data Pipeline (Airflow DAG)

This repository contains a fully automated data pipeline built with **Apache Airflow** to extract, clean, analyze, and report real estate listings from **Zoopla**.  
It pushes data to **MongoDB**, **Elasticsearch**, and **Google Sheets**, with real-time Slack alerts for monitoring.

---

## ğŸš€ Features

âœ… **Scheduled weekly** to collect and process new listings  
âœ… Cleans raw data and syncs with existing records  
âœ… Tracks **new**, **deleted**, and **updated** listings  
âœ… Analyzes market statistics (prices, turnover, etc.)  
âœ… Sends **Slack alerts** for start, success, and failure  
âœ… Delivers analytics to **Elasticsearch** and **Google Sheets**

---

## ğŸ§  Technologies Used

- Apache Airflow 2 / 3 (Astro Runtime)
- Python
- MongoDB
- Elasticsearch
- Google Sheets API
- Slack Webhook Integration
- Scrapy

---

## ğŸ“¦ Pipeline Architecture
```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      DAG Starts            â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    [Slack: Notify_Start]
                              â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Delete_Old_Records        â”‚
                 â”‚  (MongoDB Raw_Data)        â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Running_Zoopla_Spider     â”‚
                 â”‚  (Scrapy â†’ JSON Output)    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Cleaning_Data_Validation  â”‚
                 â”‚  (To ES: stage_2_clean)     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      Compare_Data          â”‚
                 â”‚(stage_2_clean vs stage_3)  â”‚
                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                      â”‚        â”‚        â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“                    â†“                      â†“
 [Add_New_Records]   [Flag_Delete_Records]   [Update_Common_Records]
          â†“                    â†“                      â†“
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“                    â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Analyse_Records        â”‚
                â”‚ (Generate analytics data)  â”‚
                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“              â†“
         [Feed_Analytics_ES]   [Feed_Google_Sheet]
                     â†“              â†“
                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                [Slack: Notify_End]
                          â†“
                [Slack: notify_failure]
```


---

## ğŸ“… DAG Schedule

- **Runs Weekly**  
- **CRON:** `30 12 * * 0` â†’ Every **Sunday at 12:30 PM UTC**

---

## ğŸ›  Setup & Deployment

> ğŸ”’ **Note:** All credentials (Slack webhook, MongoDB, ES, Sheets API) are managed via Airflow **Connections/Variables**. No secrets are stored in this code.

1. Clone the repo:
   ```bash
   git clone https://github.com/your-username/zoopla-pipeline.git
   ```

2. Add environment connections in Airflow UI:
   - `slack_webhook`
   - `mongo_conn`
   - `es_conn`
   - `google_sheets_conn`

3. Deploy the DAG to your Airflow environment (Astro, Local, or Cloud).

---

## ğŸ“Š Output

- **Elasticsearch Indices**
  - `prod_zoopla_stage_2_clean`
  - `prod_zoopla_stage_3_properties_all`
  - `prod_zoopla_stage_4_analysis`

- **Google Sheet Report**
  - Daily snapshot of analysis results

---

## ğŸ“¸ Screenshots

> *(Optional: Add screenshots of your Airflow DAG or Google Sheet report)*

---

## ğŸ¤ About the Author

**ğŸ‘¨â€ğŸ’» Ramez Rasmy**  
Freelance Data Engineer | Airflow, Python, Docker, Scraping  
ğŸ“« Contact: [Upwork Profile]([https://www.upwork.com/freelancers/~yourprofile](https://upwork.com/freelancers/ramezr))

