# ğŸ¡ Zoopla Property Data Pipeline (Airflow DAG)

This repository contains a fully automated data pipeline built with **Apache Airflow** to extract, clean, analyze, and report real estate listings from **Zoopla**.  
It pushes data to **MongoDB**, **Elasticsearch**, and **Google Sheets**, with real-time Slack alerts for monitoring.

---

## ğŸš€ Features

âœ… **Scheduled weekly** to collect and process new listings  
âœ… Cleans raw data and syncs with existing records  
âœ… Tracks **new**, **deleted**, and **updated** listings  
âœ… Analyzes market statistics (prices, turnover, etc.)  
âœ… Sends **Slack alerts** for start, success, and failure  
âœ… Delivers analytics to **Elasticsearch** and **Google Sheets**

---

## ğŸ§  Technologies Used

- Apache Airflow 2 / 3 (Astro Runtime)
- Python
- MongoDB
- Elasticsearch
- Google Sheets API
- Slack Webhook Integration
- Scrapy

---

## ğŸ“¦ Pipeline Architecture

```
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Spider   â”‚ â†â”€â”€ Scrapy scraper (Zoopla)
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   [MongoDB: Raw_Data]
          â†“
     Cleaning Step
          â†“
[Elasticsearch: stage_2_clean]
          â†“
     Comparison Step â”€â”€â–¶ Add New Records
        â”‚              â”€â”€â–¶ Flag Deleted Records
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Update Existing Records
          
          â†“
   Analytics Generation
        â†“
[Elasticsearch: stage_4_analysis]
        â†“
[Google Sheets Report]
```

---

## ğŸ“… DAG Schedule

- **Runs Weekly**  
- **CRON:** `30 12 * * 0` â†’ Every **Sunday at 12:30 PM UTC**

---

## ğŸ›  Setup & Deployment

> ğŸ”’ **Note:** All credentials (Slack webhook, MongoDB, ES, Sheets API) are managed via Airflow **Connections/Variables**. No secrets are stored in this code.

1. Clone the repo:
   ```bash
   git clone https://github.com/your-username/zoopla-pipeline.git
   ```

2. Add environment connections in Airflow UI:
   - `slack_webhook`
   - `mongo_conn`
   - `es_conn`
   - `google_sheets_conn`

3. Deploy the DAG to your Airflow environment (Astro, Local, or Cloud).

---

## ğŸ“Š Output

- **Elasticsearch Indices**
  - `prod_zoopla_stage_2_clean`
  - `prod_zoopla_stage_3_properties_all`
  - `prod_zoopla_stage_4_analysis`

- **Google Sheet Report**
  - Daily snapshot of analysis results

---

## ğŸ“¸ Screenshots

> *(Optional: Add screenshots of your Airflow DAG or Google Sheet report)*

---

## ğŸ¤ About the Author

**ğŸ‘¨â€ğŸ’» Ramez Rasmy**  
Freelance Data Engineer | Airflow, Python, Docker, Scraping  
ğŸ“« Contact: [Upwork Profile]([https://www.upwork.com/freelancers/~yourprofile](https://upwork.com/freelancers/ramezr))

